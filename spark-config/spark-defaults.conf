# Use YARN as the cluster manager
spark.master                              yarn

# Deploy mode: client (driver runs in the submitting container)
spark.submit.deployMode                   client

# Driver should be reachable from other containers
spark.driver.bindAddress                  0.0.0.0

# Hadoop filesystem (must match your namenode service)
spark.hadoop.fs.defaultFS                 hdfs://namenode:9000

# Spark SQL warehouse directory (can be local or HDFS)
spark.sql.warehouse.dir                   /tmp/spark-warehouse

# Adaptive Query Execution
spark.sql.adaptive.enabled                true
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.adaptive.skewJoin.enabled       true
spark.sql.adaptive.localShuffleReader.enabled true

# Serializer
spark.serializer                          org.apache.spark.serializer.KryoSerializer

# Arrow (disabled unless using pandas UDFs)
spark.sql.execution.arrow.pyspark.enabled false

# Memory settings (must fit within YARN limits)
spark.driver.memory                       2g
spark.driver.maxResultSize                1g
spark.executor.memory                     2g

# Network tuning
spark.network.timeout                     800s
spark.executor.heartbeatInterval          60s

# History Server Logging
spark.eventLog.enabled                    true
spark.eventLog.dir                        hdfs:///spark-events

# YARN integration: point to jars in HDFS (ensure these exist!)
# Comment out if you haven't uploaded jars yet
# spark.yarn.jars                           hdfs:///spark-jars/*