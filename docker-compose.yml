services:
  mysql:
    image: mysql:8.0
    container_name: mysql
    environment:
      MYSQL_ROOT_PASSWORD: airflow
      MYSQL_DATABASE: airflow
      MYSQL_USER: airflow
      MYSQL_PASSWORD: airflow
    ports:
      - "3307:3306"
    command: --host_cache_size=0 --pid-file=/var/lib/mysql/mysqld.pid
    volumes:
      - /usr/share/zoneinfo:/usr/share/zoneinfo:ro
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      retries: 5
      start_period: 5s
    networks:
      - airflow

  airflow-init:
    image: my_airflow:1.0.0
    container_name: airflow-init
    entrypoint: /bin/bash -c "airflow db migrate && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com"
    depends_on:
      mysql:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+mysqldb://airflow:airflow@mysql:3306/airflow
      # AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.session,airflow.api.auth.backend.session,airflow.api.auth.backend.basic_auth


    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./include:/opt/airflow/include
    networks:
      - airflow

  webserver:
    image: my_airflow:1.0.0
    container_name: webserver
    command: webserver
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    ports:
      - "8080:8080"
    environment:
      
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+mysqldb://airflow:airflow@mysql:3306/airflow
      # AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.session,airflow.api.auth.backend.basic_auth
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
      YARN_CONF_DIR: /opt/hadoop/etc/hadoop
      SPARK_HOME: /opt/bitnami/spark

    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./include:/opt/airflow/include
      - spark_home:/opt/bitnami/spark
      - ./spark-config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    networks:
      - airflow
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  scheduler:
    image: my_airflow:1.0.0
    container_name: scheduler
    command: scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+mysqldb://airflow:airflow@mysql:3306/airflow
      # AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.session,airflow.api.auth.backend.basic_auth
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
      YARN_CONF_DIR: /opt/hadoop/etc/hadoop
      SPARK_HOME: /opt/bitnami/spark

    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./include:/opt/airflow/include
      - ./hadoop-config:/opt/hadoop/etc/hadoop
      - spark_home:/opt/bitnami/spark
      - ./spark-config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    networks:
      - airflow
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname \"$${HOSTNAME}\""]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

networks:
  airflow:
    driver: bridge

volumes:
  spark_home: