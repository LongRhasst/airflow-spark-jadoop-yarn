#!/usr/bin/env python3
"""
Test Spark cluster connectivity by submitting jobs to the Spark master
"""
import requests
import json
import time
import subprocess
import os

# def test_spark_master_connection():
#     """Test connection to Spark master"""
#     print("ğŸ” Testing Spark Master connection...")
    
#     try:
#         response = requests.get("http://spark-master:8080/json/", timeout=10)
#         if response.status_code == 200:
#             print("âœ… Spark Master is accessible")
#             cluster_info = response.json()
#             active_apps = cluster_info.get('activeapps', [])
#             print(f"ğŸ“Š Found {len(active_apps)} active applications")
#             print(f"ğŸ“Š Cluster status: {cluster_info.get('status', 'UNKNOWN')}")
#             print(f"ğŸ“Š Alive workers: {cluster_info.get('aliveworkers', 0)}")
#             return True
#         else:
#             print(f"âŒ Spark Master returned status code: {response.status_code}")
#             return False
#     except Exception as e:
#         print(f"âŒ Failed to connect to Spark Master: {e}")
#         return False

def test_yarn_resourcemanager():
    """Test connection to YARN ResourceManager"""
    print("ğŸ” Testing YARN ResourceManager connection...")
    
    try:
        response = requests.get("http://resourcemanager:8088/ws/v1/cluster/info", timeout=10)
        if response.status_code == 200:
            print("âœ… YARN ResourceManager is accessible")
            info = response.json()
            cluster_info = info.get('clusterInfo', {})
            print(f"ğŸ“Š Cluster State: {cluster_info.get('state', 'Unknown')}")
            print(f"ğŸ“Š YARN Version: {cluster_info.get('hadoopVersion', 'Unknown')}")
            return True
        else:
            print(f"âŒ YARN ResourceManager returned status code: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ Failed to connect to YARN ResourceManager: {e}")
        return False

def test_hadoop_namenode():
    """Test connection to Hadoop NameNode"""
    print("ğŸ” Testing Hadoop NameNode connection...")
    
    try:
        response = requests.get("http://namenode:9870/webhdfs/v1/?op=LISTSTATUS", timeout=10)
        if response.status_code == 200:
            print("âœ… Hadoop NameNode is accessible")
            return True
        else:
            print(f"âŒ Hadoop NameNode returned status code: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ Failed to connect to Hadoop NameNode: {e}")
        return False

def submit_test_spark_job():
    """Submit a simple test job to Spark cluster"""
    print("ğŸš€ Submitting test Spark job...")
    
    # Create a simple Python script for testing
    test_script = """
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("TestSparkConnectivity") \
    .getOrCreate()

# Create a simple DataFrame
data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
columns = ["Name", "Age"]
df = spark.createDataFrame(data, columns)

print("âœ… Test DataFrame created successfully!")
df.show()

print("ğŸ“Š Total records:", df.count())
print("ğŸ‰ Spark job completed successfully!")

spark.stop()
"""
    
    # Write test script to a file
    script_path = "/tmp/test_spark_job.py"
    with open(script_path, 'w') as f:
        f.write(test_script)
    
    try:
        # Submit job to Spark cluster using PySpark directly
        # Instead of trying to use docker exec, we'll use PySpark's cluster mode
        from pyspark.sql import SparkSession
        
        print("ğŸ“‹ Submitting job to Spark cluster...")
        
        # Create Spark session that connects to the cluster
        spark = SparkSession.builder \
            .appName("ClusterTest") \
            .master("yarn") \
            .config("spark.executor.memory", "512m") \
            .config("spark.executor.cores", "1") \
            .config("spark.cores.max", "1") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "false") \
            .getOrCreate()

        
        spark.sparkContext.setLogLevel("ERROR")
        
        # Execute the test logic directly
        data = [1, 2, 3, 4, 5]
        rdd = spark.sparkContext.parallelize(data)
        result_data = rdd.map(lambda x: x * 2).collect()
        
        spark.stop()
        
        # Mock result for compatibility with the rest of the test
        class MockResult:
            def __init__(self, returncode, stdout, stderr):
                self.returncode = returncode
                self.stdout = stdout
                self.stderr = stderr
        
        result = MockResult(0, f"Test completed successfully. Results: {result_data}", "")
        
        if result.returncode == 0:
            print("âœ… Spark job submitted and completed successfully!")
            print("ğŸ“‹ Job output:")
            print(result.stdout)
            return True
        else:
            print("âŒ Spark job failed!")
            print("ğŸ“‹ Error output:")
            print(result.stderr)
            return False
            
    except subprocess.TimeoutExpired:
        print("â° Spark job timed out after 60 seconds")
        return False
    except Exception as e:
        print(f"âŒ Failed to submit Spark job: {e}")
        return False

def run_comprehensive_test():
    """Run comprehensive connectivity tests"""
    print("ğŸš€ Starting comprehensive Spark cluster connectivity tests...\n")
    
    tests = [
        # ("Spark Master", test_spark_master_connection),
        ("YARN ResourceManager", test_yarn_resourcemanager),
        ("Hadoop NameNode", test_hadoop_namenode),
        ("Spark Job Submission", submit_test_spark_job)
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        print(f"ğŸ§ª Running {test_name} test...")
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"âŒ {test_name} test failed with exception: {e}")
            results[test_name] = False
        print("-" * 50)
    
    # Summary
    print("\nğŸ“Š Test Summary:")
    passed = 0
    total = len(results)
    
    for test_name, result in results.items():
        status = "âœ… PASSED" if result else "âŒ FAILED"
        print(f"   {test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\nğŸ¯ Overall Result: {passed}/{total} tests passed")
    
    if passed == total:
        print("ğŸ‰ All tests passed! Spark cluster is ready for use.")
        return True
    else:
        print("âš ï¸  Some tests failed. Check the logs above for details.")
        return False

if __name__ == "__main__":
    success = run_comprehensive_test()
    exit(0 if success else 1)
