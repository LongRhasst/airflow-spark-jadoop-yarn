#!/usr/bin/env python3
"""
Simple test to verify Spark cluster is working by checking running applications
"""
import requests
import json

def test_spark_master_applications():
    """Test Spark Master applications endpoint"""
    print("üîç Testing Spark Master applications...")
    
    try:
        # Try the Spark REST API
        response = requests.get("http://spark-master:8080/api/v1/applications", timeout=10)
        if response.status_code == 200:
            try:
                apps = response.json()
                print(f"‚úÖ Spark Master API is working - Found {len(apps)} applications")
                return True
            except json.JSONDecodeError:
                print("‚ö†Ô∏è  Spark Master is accessible but API returned invalid JSON")
                return True  # Still consider this a success for basic connectivity
        else:
            print(f"‚ùå Spark Master API returned status code: {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ùå Failed to connect to Spark Master API: {e}")
        return False

def test_spark_workers():
    """Test Spark Workers endpoint"""
    print("üîç Testing Spark Workers...")
    
    try:
        # Check if worker is registered with master
        response = requests.get("http://spark-master:8080/api/v1/applications", timeout=10)
        if response.status_code == 200:
            print("‚úÖ Spark Master is responding to API requests")
            
            # Try to get master information
            master_response = requests.get("http://spark-master:8080/", timeout=10)
            if master_response.status_code == 200:
                print("‚úÖ Spark Master web UI is accessible")
                if "Workers" in master_response.text:
                    print("‚úÖ Spark Workers section found in web UI")
                    return True
                else:
                    print("‚ö†Ô∏è  No workers section found, but master is running")
                    return True
            else:
                print("‚ùå Spark Master web UI not accessible")
                return False
        else:
            print(f"‚ùå Spark Master returned status code: {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ùå Failed to test Spark Workers: {e}")
        return False

def test_yarn_cluster_status():
    """Test YARN cluster status"""
    print("üîç Testing YARN cluster status...")
    
    try:
        response = requests.get("http://resourcemanager:8088/ws/v1/cluster/info", timeout=10)
        if response.status_code == 200:
            info = response.json()
            cluster_info = info.get('clusterInfo', {})
            state = cluster_info.get('state', 'Unknown')
            total_memory = cluster_info.get('totalMB', 0)
            available_memory = cluster_info.get('availableMB', 0)
            
            print(f"‚úÖ YARN ResourceManager is accessible")
            print(f"üìä Cluster State: {state}")
            print(f"üìä Total Memory: {total_memory} MB")
            print(f"üìä Available Memory: {available_memory} MB")
            
            if state == "STARTED" and total_memory > 0:
                print("‚úÖ YARN cluster appears to be healthy")
                return True
            else:
                print("‚ö†Ô∏è  YARN cluster may have issues")
                return False
        else:
            print(f"‚ùå YARN ResourceManager returned status code: {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ùå Failed to test YARN cluster: {e}")
        return False

def test_hdfs_status():
    """Test HDFS status"""
    print("üîç Testing HDFS status...")
    
    try:
        # Test HDFS NameNode status
        response = requests.get("http://hadoop-namenode:9870/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem", timeout=10)
        if response.status_code == 200:
            print("‚úÖ HDFS NameNode JMX endpoint is accessible")
            return True
        else:
            # Fallback to basic connectivity test
            response = requests.get("http://hadoop-namenode:9870/", timeout=10)
            if response.status_code == 200:
                print("‚úÖ HDFS NameNode web UI is accessible")
                return True
            else:
                print(f"‚ùå HDFS NameNode returned status code: {response.status_code}")
                return False
    except Exception as e:
        print(f"‚ùå Failed to test HDFS: {e}")
        return False

def main():
    """Main test function"""
    print("üöÄ Starting Spark cluster health tests...\n")
    
    tests = [
        ("Spark Master Applications", test_spark_master_applications),
        ("Spark Workers", test_spark_workers),
        ("YARN Cluster Status", test_yarn_cluster_status),
        ("HDFS Status", test_hdfs_status)
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        print(f"üß™ Running {test_name} test...")
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"‚ùå {test_name} test failed with exception: {e}")
            results[test_name] = False
        print("-" * 50)
    
    # Summary
    print("\nüìä Test Summary:")
    passed = sum(1 for result in results.values() if result)
    total = len(results)
    
    for test_name, result in results.items():
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        print(f"   {test_name}: {status}")
    
    print(f"\nüéØ Overall Result: {passed}/{total} tests passed")
    
    if passed == total:
        print("üéâ All cluster health tests passed! The cluster is ready for Spark jobs.")
        return True
    elif passed >= total * 0.7:  # 70% pass rate
        print("‚ö†Ô∏è  Most tests passed, cluster should be functional with some limitations")
        return True
    else:
        print("‚ùå Multiple tests failed, cluster may have significant issues")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
